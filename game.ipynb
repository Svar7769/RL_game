{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "\n",
    "import gymnasium as gym  # Updated import to use gymnasium instead of gym\n",
    "from gymnasium import spaces  # Import spaces from gymnasium\n",
    "import random  # To randomly place the player, win, and lose positions\n",
    "import numpy as np  # For numerical operations and array handling\n",
    "from IPython.display import clear_output  # To clear the output display\n",
    "import os  # For operating system-related functions\n",
    "\n",
    "# Import the PPO algorithm from stable_baselines3\n",
    "from stable_baselines3 import PPO\n",
    "# For checking the custom environment\n",
    "from stable_baselines3.common.env_checker import check_env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global constants\n",
    "\n",
    "#game board values\n",
    "NOTHING = 0\n",
    "PLAYER = 1\n",
    "WIN = 2\n",
    "LOSE = 3\n",
    "\n",
    "#action values\n",
    "UP = 0\n",
    "DOWN = 1\n",
    "LEFT = 2\n",
    "RIGHT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def clear_screen():\n",
    "    clear_output()\n",
    "    os.system(\"cls\")\n",
    "\n",
    "# prints out the environment state in a visually appealing way\n",
    "\n",
    "def pretty_print(state_array, cumulative_reward):\n",
    "    clear_screen()\n",
    "    print(f'Cumulative Reward: {cumulative_reward}')\n",
    "    print()\n",
    "    for i in range(6):\n",
    "        for j in range(6):\n",
    "            print('{:4}'.format(state_array[i*6 + j]), end=\"\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom environment class inheriting from gymnasium.Env\n",
    "class BasicEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(BasicEnv, self).__init__()\n",
    "\n",
    "        # Class variable for cumulative reward\n",
    "        self.cumulative_reward = 0\n",
    "\n",
    "        # Set initial state for a flattened 6x6 grid\n",
    "        self.state = [NOTHING] * 36\n",
    "\n",
    "        # Randomly place the player, win, and lose positions\n",
    "        self.player_position = random.randrange(0, 36)\n",
    "        self.win_position = random.randrange(0, 36)\n",
    "        self.lose_position = random.randrange(0, 36)\n",
    "\n",
    "        # Ensure win and lose positions do not overlap with each other or with the player position\n",
    "        while self.win_position == self.player_position:\n",
    "            self.win_position = random.randrange(0, 36)\n",
    "        while self.lose_position == self.win_position or self.lose_position == self.player_position:\n",
    "            self.lose_position = random.randrange(0, 36)\n",
    "\n",
    "        # Update the state array with the player, win, and lose positions\n",
    "        self.state[self.player_position] = PLAYER\n",
    "        self.state[self.win_position] = WIN\n",
    "        self.state[self.lose_position] = LOSE\n",
    "\n",
    "        # Convert the state list to a numpy array\n",
    "        self.state = np.array(self.state, dtype=np.int16)\n",
    "\n",
    "        # Define the observation space (valid range for observation in the state)\n",
    "        self.observation_space = spaces.Box(0, 3, [36,], dtype=np.int16)\n",
    "\n",
    "        # Define the action space (valid actions: UP, DOWN, LEFT, RIGHT)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Placeholder for debugging information\n",
    "        info = {}\n",
    "        truncated = False\n",
    "\n",
    "        # Set default values for done, reward, and the previous player position\n",
    "        done = False\n",
    "        reward = -0.01\n",
    "        previous_position = self.player_position\n",
    "\n",
    "        # Debug print for action\n",
    "        print(f\"Action taken: {action}\")\n",
    "\n",
    "        # Check for invalid actions\n",
    "        if action == UP and (self.player_position - 6) < 0:\n",
    "            action = -1  # Invalid action\n",
    "        elif action == DOWN and (self.player_position + 6) >= 36:\n",
    "            action = -1  # Invalid action\n",
    "        elif action == LEFT and (self.player_position % 6) == 0:\n",
    "            action = -1  # Invalid action\n",
    "        elif action == RIGHT and (self.player_position % 6) == 5:\n",
    "            action = -1  # Invalid action\n",
    "\n",
    "        # If the action is valid, move the player\n",
    "        if action == UP:\n",
    "            self.player_position -= 6\n",
    "        elif action == DOWN:\n",
    "            self.player_position += 6\n",
    "        elif action == LEFT:\n",
    "            self.player_position -= 1\n",
    "        elif action == RIGHT:\n",
    "            self.player_position += 1\n",
    "        else:\n",
    "            print(\"Invalid action taken, skipping this step\")\n",
    "            return self.state, reward, done, truncated, info\n",
    "\n",
    "        # Check win or lose condition and set reward\n",
    "        if self.state[self.player_position] == WIN:\n",
    "            reward = 1.0\n",
    "            self.cumulative_reward += reward\n",
    "            done = True\n",
    "            clear_screen()\n",
    "            print(f'Cumulative Reward: {self.cumulative_reward}')\n",
    "            print('WIN !!!')\n",
    "        elif self.state[self.player_position] == LOSE:\n",
    "            reward = -1.0\n",
    "            self.cumulative_reward += reward\n",
    "            done = True\n",
    "            clear_screen()\n",
    "            print(f'Cumulative Reward: {self.cumulative_reward}')\n",
    "            print(\"Lose :B\")\n",
    "\n",
    "        # Update the environment state\n",
    "        if not done:\n",
    "            self.state[previous_position] = NOTHING\n",
    "            self.state[self.player_position] = PLAYER\n",
    "\n",
    "        self.cumulative_reward += reward\n",
    "\n",
    "        return self.state, reward, done, truncated, info\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.cumulative_reward = 0\n",
    "\n",
    "        # Set the initial state to a flattened 6x6 grid\n",
    "        self.state = [NOTHING] * 36\n",
    "\n",
    "        # Randomly place the player, win, and lose positions\n",
    "        self.player_position = random.randrange(0, 36)\n",
    "        self.win_position = random.randrange(0, 36)\n",
    "        self.lose_position = random.randrange(0, 36)\n",
    "\n",
    "        # Ensure win and lose positions do not overlap with each other or with the player position\n",
    "        while self.win_position == self.player_position:\n",
    "            self.win_position = random.randrange(0, 36)\n",
    "        while self.lose_position == self.win_position or self.lose_position == self.player_position:\n",
    "            self.lose_position = random.randrange(0, 36)\n",
    "\n",
    "        # Update the state array with the player, win, and lose positions\n",
    "        self.state[self.player_position] = PLAYER\n",
    "        self.state[self.win_position] = WIN\n",
    "        self.state[self.lose_position] = LOSE\n",
    "\n",
    "        # Convert the state list to a numpy array\n",
    "        self.state = np.array(self.state, dtype=np.int16)\n",
    "\n",
    "        return self.state, info\n",
    "\n",
    "    def render(self):\n",
    "        pretty_print(self.state, self.cumulative_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m env \u001b[39m=\u001b[39m BasicEnv()\n\u001b[0;32m      4\u001b[0m \u001b[39m# Check if the environment follows the gymnasium API\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m check_env(env)\n\u001b[0;32m      7\u001b[0m \u001b[39m# Create the PPO agent\u001b[39;00m\n\u001b[0;32m      8\u001b[0m model \u001b[39m=\u001b[39m PPO(\u001b[39m\"\u001b[39m\u001b[39mMlpPolicy\u001b[39m\u001b[39m\"\u001b[39m, env, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\stable_baselines3\\common\\env_checker.py:433\u001b[0m, in \u001b[0;36mcheck_env\u001b[1;34m(env, warn, skip_render_check)\u001b[0m\n\u001b[0;32m    430\u001b[0m action_space \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\n\u001b[0;32m    432\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 433\u001b[0m     env\u001b[39m.\u001b[39;49mreset(seed\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m    434\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    435\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe reset() method must accept a `seed` parameter\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 123\u001b[0m, in \u001b[0;36mBasicEnv.reset\u001b[1;34m(self, seed, options)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[39m# Convert the state list to a numpy array\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint16)\n\u001b[1;32m--> 123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, info\n",
      "\u001b[1;31mNameError\u001b[0m: name 'info' is not defined"
     ]
    }
   ],
   "source": [
    "# Instantiate the environment\n",
    "env = BasicEnv()\n",
    "\n",
    "# Check if the environment follows the gymnasium API\n",
    "check_env(env)\n",
    "\n",
    "# Create the PPO agent\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Save the trained agent\n",
    "model.save(\"ppo_basic_env\")\n",
    "\n",
    "# Load the trained agent\n",
    "model = PPO.load(\"ppo_basic_env\")\n",
    "\n",
    "# Visualize the current state of the environment\n",
    "env.render()\n",
    "\n",
    "# Test the agent\n",
    "obs, info = env.reset()\n",
    "for i in range(50):\n",
    "    # Predict the action to take based on the current observation\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, truncate, info = env.step(\n",
    "        action)  # Take the action in the environment\n",
    "    env.render()  # Render the environment to visualize the current state\n",
    "    if dones:  # If the episode is done, break the loop\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 21\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m# Play the game\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m play_game(model, env)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Test the agent\n",
    "def play_game(model, env, episodes=1, steps=50):\n",
    "    for episode in range(episodes):\n",
    "        obs, info = env.reset()\n",
    "        env.render()\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done and step < steps:\n",
    "            # Predict the action to take based on the current observation\n",
    "            action, _states = model.predict(obs)\n",
    "            obs, rewards, done, truncate, info = env.step(\n",
    "                action)  # Take the action in the environment\n",
    "            env.render()  # Render the environment to visualize the current state\n",
    "            step += 1\n",
    "            if done:\n",
    "                print(f\"Episode {episode + 1} finished after {step} steps\")\n",
    "                break\n",
    "\n",
    "\n",
    "# Play the game\n",
    "play_game(model, env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
