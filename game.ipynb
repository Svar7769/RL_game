{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "\n",
    "from gym import Env\n",
    "from gym import spaces\n",
    "import random\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global constants\n",
    "\n",
    "#game board values\n",
    "NOTHING = 0\n",
    "PLAYER = 1\n",
    "WIN = 2\n",
    "LOSE = 3\n",
    "\n",
    "#action values\n",
    "UP = 0\n",
    "DOWN = 1\n",
    "LEFT = 2\n",
    "RIGHT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def clear_screen():\n",
    "    clear_output()\n",
    "    os.system(\"cls\")\n",
    "\n",
    "# prints out the environment state in a visually appealing way\n",
    "\n",
    "def pretty_print(state_array, cumulative_reward):\n",
    "    clear_screen()\n",
    "    print(f'Cumulative Reward: {cumulative_reward}')\n",
    "    print()\n",
    "    for i in range(6):\n",
    "        for j in range(6):\n",
    "            print('{:4}'.format(state_array[i*6 + j]), end=\"\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicEnv(Env):\n",
    "    def __init__(self):\n",
    "        #class variable for reward\n",
    "        self.cumulative_reward = 0\n",
    "\n",
    "        #set initial state for flatten 6x6 \n",
    "        self.state = [NOTHING] * 36\n",
    "\n",
    "        self.player_position = random.randrange(0, 36)\n",
    "        self.win_position = random.randrange(0, 36)\n",
    "        self.lose_position = random.randrange(0, 36)\n",
    "\n",
    "        #makeing sure wining and loseing does not overlap each other\n",
    "        while self.win_position == self.player_position:\n",
    "            self.win_position = random.randrange(0, 36)\n",
    "\n",
    "        while self.lose_position == self.win_position or self.lose_position == self.player_position:\n",
    "            self.lose_position = random.randrange(0,36)\n",
    "\n",
    "        self.state[self.player_position] = PLAYER\n",
    "        self.state[self.win_position] = WIN\n",
    "        self.state[self.lose_position] = LOSE\n",
    "\n",
    "        # convert the python array into numpy array\n",
    "        self.state = np.array(self.state, dtype = np.int16)\n",
    "\n",
    "        # observation space (valid range for observation in the state)\n",
    "        self.observation_space = spaces.Box(0,3, [36,], dtype=np.int16)\n",
    "        \n",
    "        #spaces.discrete(4) is shortcut for defining action 0-3\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "    def step(self, action):\n",
    "        # placeholder for debugging information\n",
    "        info = {}\n",
    "\n",
    "        #set default values for done, reward and the player position\n",
    "        done = False\n",
    "        reward = -0.01\n",
    "        previous_position = self.player_position\n",
    "\n",
    "        # take action\n",
    "        # moves agent and prevent it from getting off grid\n",
    "        if action == UP:\n",
    "            if (self.player_position - 6) >= 0:\n",
    "                self.player_position -= 6\n",
    "        elif action == DOWN:\n",
    "            if (self.player_position + 6) < 36:\n",
    "                    self.player_position += 6\n",
    "        elif action == LEFT:\n",
    "            if (self.player_position % 6) != 0:\n",
    "                    self.player_position -= 1\n",
    "        elif action == RIGHT:\n",
    "            if (self.player_position % 6) != 0:\n",
    "                    self.player_position += 1\n",
    "        else: raise Exception(\"Invalid action\")\n",
    "\n",
    "        #check win or lose condition and set reward\n",
    "        if self.state[self.player_position] == WIN:\n",
    "            reward = 1.0\n",
    "            self.cumulative_reward += reward\n",
    "            done = True\n",
    "\n",
    "            #this section diplays purposes\n",
    "            clear_screen()\n",
    "            print(f'Cumulative Reward: {self.cumulative_reward}')\n",
    "            print('WIN !!!')\n",
    "\n",
    "        elif self.state[self.player_position] == LOSE:\n",
    "            reward = -1.0\n",
    "            self.cumulative_reward += reward\n",
    "            done = True\n",
    "\n",
    "            # this section diplays progress\n",
    "            clear_screen()\n",
    "            print(f'Cumulative Reward: {self.cumulative_reward}')\n",
    "            print(\"Lose :B\")\n",
    "\n",
    "        # Update the environment state\n",
    "\n",
    "        if not done:\n",
    "            self.state[previous_position] = NOTHING\n",
    "            self.state[self.player_position] = PLAYER\n",
    "\n",
    "        self.cumulative_reward += reward\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.cumulative_reward = 0\n",
    "\n",
    "        #set the initial state to a flatten 6X6 grid\n",
    "        self.state = [NOTHING] * 36\n",
    "\n",
    "        self.player_position = random.randrange(0,36)\n",
    "        self.win_position = random.randrange(0,36)\n",
    "        self.lose_position = random.randrange(0,36)\n",
    "\n",
    "        # making sure entry and lose points are not overlapping each other\n",
    "        while self.win_position == self.player_position:\n",
    "            self.win_position = random.randrange(0,36)\n",
    "\n",
    "        while self.lose_position == self.win_position or self.lose_position == self.player_position:\n",
    "            self.lose_position = random.randrange(0,36)\n",
    "\n",
    "        self.state[self.player_position] = PLAYER\n",
    "        self.state[self.win_position] = WIN\n",
    "        self.state[self.lose_position] = LOSE\n",
    "\n",
    "        # convert the python array to numpy array\n",
    "        self.state = np.array(self.state, dtype=np.int16)\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    def render(self):\n",
    "        pretty_print(self.state, self.cumulative_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m action \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[39m# after action this prodes env information\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m state, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     13\u001b[0m \u001b[39m#keep repeating until the game is over\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# running env \n",
    "env = BasicEnv()\n",
    "\n",
    "# visualize the current state of the environment\n",
    "env.render() \n",
    "\n",
    "# ask user for action\n",
    "action = 1\n",
    "\n",
    "# after action this prodes env information\n",
    "state, reward, done, info = env.step(action)\n",
    "\n",
    "#keep repeating until the game is over\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = random.randrange(0,3)\n",
    "    state, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Your environment must inherit from the gymnasium.Env class cf. https://gymnasium.farama.org/api/env/",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines3\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menv_checker\u001b[39;00m \u001b[39mimport\u001b[39;00m check_env\n\u001b[0;32m      2\u001b[0m env \u001b[39m=\u001b[39m BasicEnv()\n\u001b[1;32m----> 3\u001b[0m check_env(env)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\stable_baselines3\\common\\env_checker.py:421\u001b[0m, in \u001b[0;36mcheck_env\u001b[1;34m(env, warn, skip_render_check)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_env\u001b[39m(env: gym\u001b[39m.\u001b[39mEnv, warn: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, skip_render_check: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    407\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[39m    Check that an environment follows Gym API.\u001b[39;00m\n\u001b[0;32m    409\u001b[0m \u001b[39m    This is particularly useful when using a custom environment.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[39m        True by default (useful for the CI)\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 421\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m    422\u001b[0m         env, gym\u001b[39m.\u001b[39mEnv\n\u001b[0;32m    423\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mYour environment must inherit from the gymnasium.Env class cf. https://gymnasium.farama.org/api/env/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    425\u001b[0m     \u001b[39m# ============= Check the spaces (observation and action) ================\u001b[39;00m\n\u001b[0;32m    426\u001b[0m     _check_spaces(env)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Your environment must inherit from the gymnasium.Env class cf. https://gymnasium.farama.org/api/env/"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "env = BasicEnv()\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
