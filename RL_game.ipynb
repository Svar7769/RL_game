{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRNx08m/fvD0JXnmBtkTB+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Svar7769/RL_game/blob/main/RL_game.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7E5dWhCvkDmX",
        "outputId": "14abbd58-9a66-4d74-9bf5-c2a60e9b7740"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMYicehkmPJp",
        "outputId": "46ed17b1-bf73-4c1a-b3d1-02fde3860452"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cumulative Reward: -2.0700000000000003\n",
            "\n",
            "   0   0   0   0   0   2\n",
            "   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0\n",
            "   1   3   0   0   0   0\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import gymnasium as gym  # Updated import to use gymnasium instead of gym\n",
        "from gymnasium import spaces  # Import spaces from gymnasium\n",
        "import random  # To randomly place the player, win, and lose positions\n",
        "import numpy as np  # For numerical operations and array handling\n",
        "from IPython.display import clear_output  # To clear the output display\n",
        "import os  # For operating system-related functions\n",
        "\n",
        "from stable_baselines3 import PPO  # Import the PPO algorithm from stable_baselines3\n",
        "from stable_baselines3.common.env_checker import check_env  # For checking the custom environment\n",
        "\n",
        "# Global constants for game board values\n",
        "NOTHING = 0  # Represents an empty space on the board\n",
        "PLAYER = 1  # Represents the player's position on the board\n",
        "WIN = 2  # Represents the win position on the board\n",
        "LOSE = 3  # Represents the lose position on the board\n",
        "\n",
        "# Action values\n",
        "UP = 0\n",
        "DOWN = 1\n",
        "LEFT = 2\n",
        "RIGHT = 3\n",
        "\n",
        "# Helper function to clear the screen\n",
        "def clear_screen():\n",
        "    clear_output(wait=True)  # Clear the output display in Jupyter Notebook\n",
        "    os.system(\"cls\" if os.name == \"nt\" else \"clear\")  # Clear the terminal screen for Windows or Unix-based systems\n",
        "\n",
        "# Function to print the environment state in a visually appealing way\n",
        "def pretty_print(state_array, cumulative_reward):\n",
        "    clear_screen()  # Clear the screen before printing the new state\n",
        "    print(f'Cumulative Reward: {cumulative_reward}')  # Print the cumulative reward\n",
        "    print()\n",
        "    for i in range(6):  # Loop through each row\n",
        "        for j in range(6):  # Loop through each column\n",
        "            print('{:4}'.format(state_array[i*6 + j]), end=\"\")  # Print each cell value with a width of 4\n",
        "        print()  # Newline at the end of each row\n",
        "\n",
        "# Define the custom environment class inheriting from gymnasium.Env\n",
        "class BasicEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(BasicEnv, self).__init__()\n",
        "\n",
        "        # Class variable for cumulative reward\n",
        "        self.cumulative_reward = 0\n",
        "\n",
        "        # Set initial state for a flattened 6x6 grid\n",
        "        self.state = [NOTHING] * 36\n",
        "\n",
        "        # Randomly place the player, win, and lose positions\n",
        "        self.player_position = random.randrange(0, 36)\n",
        "        self.win_position = random.randrange(0, 36)\n",
        "        self.lose_position = random.randrange(0, 36)\n",
        "\n",
        "        # Ensure win and lose positions do not overlap with each other or with the player position\n",
        "        while self.win_position == self.player_position:\n",
        "            self.win_position = random.randrange(0, 36)\n",
        "        while self.lose_position == self.win_position or self.lose_position == self.player_position:\n",
        "            self.lose_position = random.randrange(0, 36)\n",
        "\n",
        "        # Update the state array with the player, win, and lose positions\n",
        "        self.state[self.player_position] = PLAYER\n",
        "        self.state[self.win_position] = WIN\n",
        "        self.state[self.lose_position] = LOSE\n",
        "\n",
        "        # Convert the state list to a numpy array\n",
        "        self.state = np.array(self.state, dtype=np.int16)\n",
        "\n",
        "        # Define the observation space (valid range for observation in the state)\n",
        "        self.observation_space = spaces.Box(0, 3, [36,], dtype=np.int16)\n",
        "\n",
        "        # Define the action space (valid actions: UP, DOWN, LEFT, RIGHT)\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "\n",
        "    def step(self, action):\n",
        "        # Placeholder for debugging information\n",
        "        info = {}\n",
        "        truncated = False\n",
        "\n",
        "        # Set default values for done, reward, and the previous player position\n",
        "        done = False\n",
        "        reward = -0.01\n",
        "        previous_position = self.player_position\n",
        "\n",
        "        # Debug print for action\n",
        "        print(f\"Action taken: {action}\")\n",
        "\n",
        "        # Check for invalid actions\n",
        "        if action == UP and (self.player_position - 6) < 0:\n",
        "            action = -1  # Invalid action\n",
        "        elif action == DOWN and (self.player_position + 6) >= 36:\n",
        "            action = -1  # Invalid action\n",
        "        elif action == LEFT and (self.player_position % 6) == 0:\n",
        "            action = -1  # Invalid action\n",
        "        elif action == RIGHT and (self.player_position % 6) == 5:\n",
        "            action = -1  # Invalid action\n",
        "\n",
        "        # If the action is valid, move the player\n",
        "        if action == UP:\n",
        "            self.player_position -= 6\n",
        "        elif action == DOWN:\n",
        "            self.player_position += 6\n",
        "        elif action == LEFT:\n",
        "            self.player_position -= 1\n",
        "        elif action == RIGHT:\n",
        "            self.player_position += 1\n",
        "        else:\n",
        "            print(\"Invalid action taken, skipping this step\")\n",
        "            return self.state, reward, done, truncated, info\n",
        "\n",
        "        # Check win or lose condition and set reward\n",
        "        if self.state[self.player_position] == WIN:\n",
        "            reward = 1.0\n",
        "            self.cumulative_reward += reward\n",
        "            done = True\n",
        "            clear_screen()\n",
        "            print(f'Cumulative Reward: {self.cumulative_reward}')\n",
        "            print('WIN !!!')\n",
        "        elif self.state[self.player_position] == LOSE:\n",
        "            reward = -1.0\n",
        "            self.cumulative_reward += reward\n",
        "            done = True\n",
        "            clear_screen()\n",
        "            print(f'Cumulative Reward: {self.cumulative_reward}')\n",
        "            print(\"Lose :B\")\n",
        "\n",
        "        # Update the environment state\n",
        "        if not done:\n",
        "            self.state[previous_position] = NOTHING\n",
        "            self.state[self.player_position] = PLAYER\n",
        "\n",
        "        self.cumulative_reward += reward\n",
        "\n",
        "        return self.state, reward, done, truncated, info\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        self.cumulative_reward = 0\n",
        "\n",
        "        # Set the initial state to a flattened 6x6 grid\n",
        "        self.state = [NOTHING] * 36\n",
        "\n",
        "        # Randomly place the player, win, and lose positions\n",
        "        self.player_position = random.randrange(0, 36)\n",
        "        self.win_position = random.randrange(0, 36)\n",
        "        self.lose_position = random.randrange(0, 36)\n",
        "\n",
        "        # Ensure win and lose positions do not overlap with each other or with the player position\n",
        "        while self.win_position == self.player_position:\n",
        "            self.win_position = random.randrange(0, 36)\n",
        "        while self.lose_position == self.win_position or self.lose_position == self.player_position:\n",
        "            self.lose_position = random.randrange(0, 36)\n",
        "\n",
        "        # Update the state array with the player, win, and lose positions\n",
        "        self.state[self.player_position] = PLAYER\n",
        "        self.state[self.win_position] = WIN\n",
        "        self.state[self.lose_position] = LOSE\n",
        "\n",
        "        # Convert the state list to a numpy array\n",
        "        self.state = np.array(self.state, dtype=np.int16)\n",
        "\n",
        "        return self.state, info\n",
        "\n",
        "    def render(self):\n",
        "        pretty_print(self.state, self.cumulative_reward)\n",
        "\n",
        "# Instantiate the environment\n",
        "env = BasicEnv()\n",
        "\n",
        "# Check if the environment follows the gymnasium API\n",
        "check_env(env)\n",
        "\n",
        "# Create the PPO agent\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "\n",
        "# Train the agent\n",
        "model.learn(total_timesteps=10000)\n",
        "\n",
        "# Save the trained agent\n",
        "model.save(\"ppo_basic_env\")\n",
        "\n",
        "# Load the trained agent\n",
        "model = PPO.load(\"ppo_basic_env\")\n",
        "\n",
        "# Visualize the current state of the environment\n",
        "env.render()\n",
        "\n",
        "# Test the agent\n",
        "obs, info = env.reset()\n",
        "for i in range(50):\n",
        "    action, _states = model.predict(obs)  # Predict the action to take based on the current observation\n",
        "    obs, rewards, dones, truncate, info = env.step(action)  # Take the action in the environment\n",
        "    env.render()  # Render the environment to visualize the current state\n",
        "    if dones:  # If the episode is done, break the loop\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the agent\n",
        "def play_game(model, env, episodes=1, steps=50):\n",
        "    for episode in range(episodes):\n",
        "        obs, info = env.reset()\n",
        "        env.render()\n",
        "        done = False\n",
        "        step = 0\n",
        "        while not done and step < steps:\n",
        "            action, _states = model.predict(obs)  # Predict the action to take based on the current observation\n",
        "            obs, rewards, done, truncate, info = env.step(action)  # Take the action in the environment\n",
        "            env.render()  # Render the environment to visualize the current state\n",
        "            step += 1\n",
        "            if done:\n",
        "                print(f\"Episode {episode + 1} finished after {step} steps\")\n",
        "                break\n",
        "\n",
        "# Play the game\n",
        "play_game(model, env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPSpX3D6n3x9",
        "outputId": "be4e4538-e84e-4ed8-ddd1-8a372acf3300"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cumulative Reward: 1.74\n",
            "\n",
            "   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0\n",
            "   2   1   0   0   0   0\n",
            "   3   0   0   0   0   0\n",
            "   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0\n",
            "Episode 1 finished after 33 steps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XWBIjEjvp4yH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}